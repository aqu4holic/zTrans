{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "seed = 18022004\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prefix: str = 'data'\n",
    "repo_prefix: str = f'{data_prefix}/repos'\n",
    "\n",
    "data_name = 'data_method_30k_test.parquet'\n",
    "\n",
    "data_df: pd.DataFrame = pd.read_parquet(f'{data_prefix}/{data_name}', engine = 'pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "hf_dataset = Dataset.from_pandas(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = hf_dataset.train_test_split(test_size = 0.2, seed = seed)\n",
    "\n",
    "val_test_split = train_test_split['test'].train_test_split(test_size = 0.5, seed = seed)\n",
    "\n",
    "final_datasets = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': val_test_split['train'],\n",
    "    'test': val_test_split['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'fromLib', 'toLib', 'repoName', 'prevCommit', 'startCommit', 'endCommit', 'fileName', 'startCommitChanges', 'endCommitChanges', 'startCode_cleaned', 'endCode_cleaned', 'diff_cleaned', 'methods_before', 'methods_after', 'methods_diff'],\n",
       "        num_rows: 23800\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'fromLib', 'toLib', 'repoName', 'prevCommit', 'startCommit', 'endCommit', 'fileName', 'startCommitChanges', 'endCommitChanges', 'startCode_cleaned', 'endCode_cleaned', 'diff_cleaned', 'methods_before', 'methods_after', 'methods_diff'],\n",
       "        num_rows: 2975\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'fromLib', 'toLib', 'repoName', 'prevCommit', 'startCommit', 'endCommit', 'fileName', 'startCommitChanges', 'endCommitChanges', 'startCode_cleaned', 'endCode_cleaned', 'diff_cleaned', 'methods_before', 'methods_after', 'methods_diff'],\n",
       "        num_rows: 2976\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:03<00:00,  1.23ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:02<00:00,  1.53ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:03<00:00,  1.30ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:02<00:00,  1.39ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:03<00:00,  1.31ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:02<00:00,  1.49ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 6/6 [01:37<00:00, 16.18s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:01<00:00,  1.62ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:09<00:00,  9.93s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:02<00:00,  1.43ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:11<00:00, 11.13s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/blackwhite1337/zTrans_dataset/commit/427e42bb6c28707798e4fdb7685a1a9f08eda5b5', commit_message='Upload dataset', commit_description='', oid='427e42bb6c28707798e4fdb7685a1a9f08eda5b5', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/blackwhite1337/zTrans_dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='blackwhite1337/zTrans_dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token = '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')\n",
    "\n",
    "username = 'blackwhite1337'\n",
    "dataset_name = 'zTrans_dataset'\n",
    "\n",
    "final_datasets.push_to_hub(f'{username}/{dataset_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 23800/23800 [00:12<00:00, 1890.35 examples/s]\n",
      "Generating validation split: 100%|██████████| 2975/2975 [00:03<00:00, 949.63 examples/s] \n",
      "Generating test split: 100%|██████████| 2976/2976 [00:04<00:00, 710.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "username = 'blackwhite1337'\n",
    "dataset_name = 'zTrans_dataset'\n",
    "\n",
    "dataset = load_dataset(f'{username}/{dataset_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format:   0%|          | 0/24 [00:00<?, ?ba/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 24/24 [00:10<00:00,  2.29ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:01<00:00,  2.48ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "365139365"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['train'].to_parquet(f'{data_prefix}/data_method_30k_train.parquet')\n",
    "dataset['validation'].to_parquet(f'{data_prefix}/data_method_30k_val.parquet')\n",
    "dataset['test'].to_parquet(f'{data_prefix}/data_method_30k_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = final_datasets['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts: List[str] = []\n",
    "\n",
    "BEGIN_TOKEN: str = '<｜fim▁begin｜>'\n",
    "FILL_TOKEN: str = '<｜fim▁hole｜>'\n",
    "END_TOKEN: str = '<｜fim▁end｜>'\n",
    "\n",
    "prompt_template: str = '''// rewrite the below Java method from this library \"{}\" to this library \"{}\" in {}\n",
    "// output in this format ```method code```, just the method, no comments, no imports\n",
    "{}\n",
    "'''\n",
    "\n",
    "for id in range(len(data_df)):\n",
    "    line = data_df.iloc[id]\n",
    "\n",
    "    from_lib: str = line['fromLib']\n",
    "    to_lib: str = line['toLib']\n",
    "    method_before: str = line['methods_before']\n",
    "    file_name = line['fileName']\n",
    "\n",
    "    if (len(method_before) == 0):\n",
    "        continue\n",
    "\n",
    "    prompt: str = prompt_template.format(from_lib, to_lib, file_name, method_before)\n",
    "    ground_truth: str = line['methods_after']\n",
    "\n",
    "    prompts.append({'prompt': prompt, 'ground_truth': ground_truth})\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// rewrite the below Java method from this library \"com.fasterxml.jackson.core:jackson-annotations\" to this library \"com.google.code.gson:gson\" in system/rest-spring/src/main/java/org/platformlambda/rest/spring/system/WebsocketLoader.java\n",
      "// output in this format ```method code```, just the method, no comments, no imports\n",
      "public void contextInitialized(ServletContextEvent event) {\n",
      "        ServletContext context = event.getServletContext();\n",
      "        SimpleClassScanner scanner = SimpleClassScanner.getInstance();\n",
      "        Set<String> packages = scanner.getPackages(true);\n",
      "        Object sc = context.getAttribute(\"javax.websocket.server.ServerContainer\");\n",
      "        if (sc instanceof ServerContainer) {\n",
      "            ServerContainer container = (ServerContainer) sc;\n",
      "            int total = 0;\n",
      "            for (String p : packages) {\n",
      "                List<Class<?>> endpoints = scanner.getAnnotatedClasses(p, ServerEndpoint.class);\n",
      "                for (Class<?> cls : endpoints) {\n",
      "                    if (!WebAppLoader.isRequired(cls)) {\n",
      "                        continue;\n",
      "                    }\n",
      "                    try {\n",
      "                        container.addEndpoint(cls);\n",
      "                        ServerEndpoint ep = cls.getAnnotation(ServerEndpoint.class);\n",
      "                        total++;\n",
      "                        log.info(\"{} registered as WEBSOCKET DISPATCHER {}\", cls.getName(), Arrays.asList(ep.value()));\n",
      "                    } catch (DeploymentException e) {\n",
      "                        log.error(\"Unable to deploy websocket endpoint {} - {}\", cls, e.getMessage());\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "            if (total > 0) {\n",
      "                log.info(\"Total {} Websocket server endpoint{} registered (JSR-356)\", total, total == 1 ? \" is\" : \"s are\");\n",
      "            }\n",
      "        } else {\n",
      "            log.error(\"Unable to register any ServerEndpoints because javax.websocket.server.ServerContainer is not available\");\n",
      "        }\n",
      "    }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_id: str = 'deepseek-ai/deepseek-coder-6.7b-instruct'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code = True,)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit = True,\n",
    ")\n",
    "\n",
    "device: str = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code = True,\n",
    "    quantization_config = quantization_config,\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map = 'auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = tokenizer.encode(prompt, return_tensors = 'pt').to(model.device)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': prompts[0]['prompt'],\n",
    "    },\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt = True, return_tensors = 'pt').to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/drive2/phatnt/zTrans/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens = 512,\n",
    "    do_sample = False,\n",
    "    top_k = 50,\n",
    "    top_p = 0.95,\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the equivalent code using Gson library for the same functionality:\n",
      "\n",
      "```java\n",
      "import com.google.gson.Gson;\n",
      "\n",
      "public void contextInitialized(ServletContextEvent event) {\n",
      "    ServletContext context = event.getServletContext();\n",
      "    SimpleClassScanner scanner = SimpleClassScanner.getInstance();\n",
      "    Set<String> packages = scanner.getPackages(true);\n",
      "    Object sc = context.getAttribute(\"javax.websocket.server.ServerContainer\");\n",
      "    if (sc instanceof ServerContainer) {\n",
      "        ServerContainer container = (ServerContainer) sc;\n",
      "        int total = 0;\n",
      "        for (String p : packages) {\n",
      "            List<Class<?>> endpoints = scanner.getAnnotatedClasses(p, ServerEndpoint.class);\n",
      "            for (Class<?> cls : endpoints) {\n",
      "                if (!WebAppLoader.isRequired(cls)) {\n",
      "                    continue;\n",
      "                }\n",
      "                try {\n",
      "                    container.addEndpoint(cls);\n",
      "                    ServerEndpoint ep = cls.getAnnotation(ServerEndpoint.class);\n",
      "                    total++;\n",
      "                    log.info(\"{} registered as WEBSOCKET DISPATCHER {}\", cls.getName(), Arrays.asList(ep.value()));\n",
      "                } catch (DeploymentException e) {\n",
      "                    log.error(\"Unable to deploy websocket endpoint {} - {}\", cls, e.getMessage());\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "        if (total > 0) {\n",
      "            log.info(\"Total {} Websocket server endpoint{} registered (JSR-356)\", total, total == 1 ? \" is\" : \"s are\");\n",
      "        }\n",
      "    } else {\n",
      "        log.error(\"Unable to register any ServerEndpoints because javax.websocket.server.ServerContainer is not available\");\n",
      "    }\n",
      "}\n",
      "```\n",
      "\n",
      "Please note that Gson library is not part of the standard Java libraries, so you need to add it to your project dependencies.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output_text = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens = True,)\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public void contextInitialized(ServletContextEvent event) {\n",
      "        ServletContext context = event.getServletContext();\n",
      "        SimpleClassScanner scanner = SimpleClassScanner.getInstance();\n",
      "        Set<String> packages = scanner.getPackages(true);\n",
      "        Object sc = context.getAttribute(\"javax.websocket.server.ServerContainer\");\n",
      "        if (sc instanceof ServerContainer) {\n",
      "            ServerContainer container = (ServerContainer) sc;\n",
      "            int total = 0;\n",
      "            for (String p : packages) {\n",
      "                List<Class<?>> endpoints = scanner.getAnnotatedClasses(p, ServerEndpoint.class);\n",
      "                for (Class<?> cls : endpoints) {\n",
      "                    if (!Feature.isRequired(cls)) {\n",
      "                        continue;\n",
      "                    }\n",
      "                    try {\n",
      "                        container.addEndpoint(cls);\n",
      "                        ServerEndpoint ep = cls.getAnnotation(ServerEndpoint.class);\n",
      "                        total++;\n",
      "                        log.info(\"{} registered as WEBSOCKET DISPATCHER {}\", cls.getName(), Arrays.asList(ep.value()));\n",
      "                    } catch (DeploymentException e) {\n",
      "                        log.error(\"Unable to deploy websocket endpoint {} - {}\", cls, e.getMessage());\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "            if (total > 0) {\n",
      "                log.info(\"Total {} Websocket server endpoint{} registered (JSR-356)\", total, total == 1 ? \" is\" : \"s are\");\n",
      "            }\n",
      "        } else {\n",
      "            log.error(\"Unable to register any ServerEndpoints because javax.websocket.server.ServerContainer is not available\");\n",
      "        }\n",
      "    }\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0]['ground_truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "diff = difflib.unified_diff(prompt.splitlines(), output_text.splitlines(), lineterm = '')\n",
    "\n",
    "print('\\n'.join(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
