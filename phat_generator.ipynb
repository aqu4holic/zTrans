{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, set_seed\n",
    "import datasets\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Any, Callable\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'\n",
    "\n",
    "seed = 18022004\n",
    "np.random.seed(seed)\n",
    "set_seed(seed)\n",
    "\n",
    "data_prefix: str = 'data'\n",
    "repo_prefix: str = f'{data_prefix}/repos'\n",
    "\n",
    "prompt_template: str = '''rewrite below method from library \"{}\" to \"{}\". ONLY WRITE CODE, NO COMMENTS, IMPORTS, TEXT, NO EXPLAIN.\n",
    "```\n",
    "{}\n",
    "```\n",
    "'''\n",
    "\n",
    "batch_prompt_template: str = '''<｜begin▁of▁sentence｜>### Instruction:\n",
    "you're a software engineer working on a project. ONLY RESPOND WITH CODE, NO COMMENTS, IMPORTS, TEXT, NO EXPLAIN.\n",
    "rewrite below method from library \"{}\" to \"{}\".\n",
    "```\n",
    "{}\n",
    "```\n",
    "\n",
    "### Response:\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_time(function: Callable, *args, **kwargs) -> Any:\n",
    "    start_time: float = time.time()\n",
    "    result = function(*args, **kwargs)\n",
    "    end_time: float = time.time()\n",
    "    print(f'Executed {function.__name__} in {end_time - start_time} seconds')\n",
    "    print('-' * 50)\n",
    "\n",
    "    return result\n",
    "\n",
    "def build_prompts(data_df: pd.DataFrame, batched: bool) -> List[Any]:\n",
    "    prompts: List[Any] = []\n",
    "\n",
    "    BEGIN_TOKEN: str = '<｜fim▁begin｜>'\n",
    "    FILL_TOKEN: str = '<｜fim▁hole｜>'\n",
    "    END_TOKEN: str = '<｜fim▁end｜>'\n",
    "\n",
    "    for id in tqdm(range(len(data_df)), desc = 'Building prompts'):\n",
    "        line = data_df.iloc[id]\n",
    "\n",
    "        from_lib: str = line['fromLib']\n",
    "        to_lib: str = line['toLib']\n",
    "        method_before: str = line['method_before']\n",
    "        ground_truth: str = line['method_after']\n",
    "\n",
    "        if (batched):\n",
    "            prompt: str = batch_prompt_template.format(from_lib, to_lib, method_before)\n",
    "        else:\n",
    "            prompt: str = prompt_template.format(from_lib, to_lib, method_before)\n",
    "        ground_truth: str = line['method_after']\n",
    "\n",
    "        prompts.append({'id': line['id'], 'prompt': prompt, 'ground_truth': ground_truth})\n",
    "\n",
    "    return prompts\n",
    "\n",
    "def build_prompts_hf(data_df: datasets.arrow_dataset.Dataset) -> List[Any]:\n",
    "    prompts: List[Any] = []\n",
    "\n",
    "    BEGIN_TOKEN: str = '<｜fim▁begin｜>'\n",
    "    FILL_TOKEN: str = '<｜fim▁hole｜>'\n",
    "    END_TOKEN: str = '<｜fim▁end｜>'\n",
    "\n",
    "    for id in tqdm(range(len(data_df)), desc = 'Building prompts'):\n",
    "        line = data_df[id]\n",
    "\n",
    "        from_lib: str = line['fromLib']\n",
    "        to_lib: str = line['toLib']\n",
    "        method_before: str = line['method_before']\n",
    "        ground_truth: str = line['method_after']\n",
    "\n",
    "        prompt: str = prompt_template.format(from_lib, to_lib, method_before)\n",
    "\n",
    "        prompts.append({'id': line['id'], 'prompt': prompt, 'ground_truth': ground_truth})\n",
    "\n",
    "    return prompts\n",
    "\n",
    "def build_tokenizer(args: argparse.Namespace) -> AutoTokenizer:\n",
    "    model_id: str = args.model\n",
    "    tokenizer: AutoTokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code = True,)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def build_model(args: argparse.Namespace) -> AutoModelForCausalLM:\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit = True,\n",
    "    )\n",
    "\n",
    "    device_id: str = args.device\n",
    "    model_id: str = args.model\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = device_id\n",
    "\n",
    "    # device: str = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        trust_remote_code = True,\n",
    "        quantization_config = quantization_config,\n",
    "        torch_dtype = torch.float16,\n",
    "        device_map = 'auto',\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def decode_outputs(tokenizer: AutoTokenizer, outputs: List[Any]) -> List[Any]:\n",
    "    results: List[Any] = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for output in tqdm(outputs, desc = 'Decoding'):\n",
    "            id = output['id']\n",
    "            single_inputs = output['inputs']\n",
    "            single_outputs = output['outputs']\n",
    "            prompt = output['prompt']\n",
    "\n",
    "            decoded_output = tokenizer.decode(single_outputs[0][len(single_inputs[0]):], skip_special_tokens = True)\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    'id': id,\n",
    "                    'output': decoded_output,\n",
    "                    'prompt': prompt,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return results\n",
    "\n",
    "def save_results(args: argparse.Namespace, results: List[str], data_df: pd.DataFrame):\n",
    "    output_name: str = args.output_file\n",
    "\n",
    "    valid_ids = [result['id'] for result in results]\n",
    "    res_df = data_df[data_df['id'].isin(valid_ids)].copy()\n",
    "\n",
    "    res_df['predicted'] = ''\n",
    "    res_df['prompt'] = ''\n",
    "\n",
    "    for id in range(len(results)):\n",
    "        sample = results[id]\n",
    "\n",
    "        res_df.loc[res_df['id'] == sample['id'], 'prompt'] = sample['prompt']\n",
    "        res_df.loc[res_df['id'] == sample['id'], 'predicted'] = sample['output']\n",
    "\n",
    "    res_df.to_parquet(f'{data_prefix}/{output_name}', engine = 'pyarrow')\n",
    "\n",
    "def build_message_inputs(prompts: List[Any], tokenizer: AutoTokenizer) -> List[Any]:\n",
    "    messages: List[Any] = []\n",
    "    valid_inputs: List[Any] = []\n",
    "\n",
    "    for id in tqdm(range(len(prompts)), desc = 'Building inputs'):\n",
    "        sample = prompts[id]\n",
    "\n",
    "        messages = [(\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': sample['prompt'],\n",
    "            }\n",
    "        )]\n",
    "\n",
    "        inputs = tokenizer.apply_chat_template(messages, add_generation_prompt = True, padding = True, truncation = True, return_tensors = 'pt').to('cpu')\n",
    "\n",
    "        valid_inputs.append({\n",
    "            'id': sample['id'],\n",
    "            'inputs': inputs,\n",
    "            'prompt': sample['prompt'],\n",
    "        })\n",
    "\n",
    "    return valid_inputs\n",
    "\n",
    "def generate_from_inputs(args: argparse.Namespace, model: AutoModelForCausalLM, tokenizer: AutoTokenizer, valid_inputs: List[Any], data_df: pd.DataFrame) -> List[Any]:\n",
    "    outputs: List[Any] = []\n",
    "\n",
    "    max_new_tokens: int = args.max_new_tokens\n",
    "    do_sample: bool = args.do_sample\n",
    "    top_k: int = args.top_k\n",
    "    top_p: float = args.top_p\n",
    "\n",
    "    for sample in tqdm(valid_inputs, desc = 'Generating'):\n",
    "        id = sample['id']\n",
    "        single_inputs = sample['inputs']\n",
    "        prompt = sample['prompt']\n",
    "\n",
    "        single_inputs = single_inputs.to(model.device)\n",
    "        single_outputs = model.generate(\n",
    "            single_inputs,\n",
    "            max_new_tokens = max_new_tokens,\n",
    "            do_sample = do_sample,\n",
    "            top_k = top_k,\n",
    "            top_p = top_p,\n",
    "            eos_token_id = tokenizer.eos_token_id,\n",
    "            pad_token_id = tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        single_inputs = single_inputs.to('cpu')\n",
    "\n",
    "        outputs.append(\n",
    "            {\n",
    "                'id': id,\n",
    "                'inputs': single_inputs,\n",
    "                'outputs': single_outputs,\n",
    "                'prompt': prompt,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # save results every 15 samples\n",
    "        if (len(outputs) % 15 == 0):\n",
    "            results: List[Any] = decode_outputs(tokenizer = tokenizer, outputs = outputs)\n",
    "\n",
    "            save_results(args = args, results = results, data_df = data_df)\n",
    "\n",
    "            print(f'saved results for {len(outputs)} samples')\n",
    "            print('-' * 50)\n",
    "            print()\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict: Dict[Any, Any] = {\n",
    "    'input_file': 'migration_others_dataset_cutoff_test_512.parquet',\n",
    "    'output_file': 'migration_others_dataset_cutoff_test_512.parquet',\n",
    "    'dataset_id': 'blackwhite1337/zTrans_dataset_512',\n",
    "    'split': 'test',\n",
    "\n",
    "    'model': 'deepseek-ai/deepseek-coder-6.7b-instruct',\n",
    "    'device': '0',\n",
    "    'batch_size': 2,\n",
    "\n",
    "    'max_length': 512,\n",
    "    'max_new_tokens': 512,\n",
    "    'do_sample': False,\n",
    "    'top_k': 50,\n",
    "    'top_p': 0.95,\n",
    "}\n",
    "\n",
    "args_list = [f'--{key}={value}' for key, value in args_dict.items()]\n",
    "\n",
    "parser = argparse.ArgumentParser(description = 'Process a file.')\n",
    "\n",
    "# data parameters\n",
    "parser.add_argument('--input_file', type = str, nargs = '?', default = 'sampled_no_code.parquet', help = 'The name of the file to process')\n",
    "parser.add_argument('--output_file', type = str, nargs = '?', default = 'sampled_code.parquet', help = 'The name of the file to output')\n",
    "parser.add_argument('--dataset_id', type = str, nargs = '?', default = 'blackwhite1337/zTrans_dataset', help = 'Dataset ID on Huggingface')\n",
    "parser.add_argument('--split', type = str, nargs = '?', default = 'test', help = 'Dataset split to use')\n",
    "\n",
    "# model parameters\n",
    "parser.add_argument('--model', type = str, nargs = '?', default = 'deepseek-ai/deepseek-coder-6.7b-instruct', help = 'Model ID on Huggingface')\n",
    "parser.add_argument('--device', nargs = '?', default = '0', help = 'GPU ID to use')\n",
    "parser.add_argument('--batch_size', type = int, nargs = '?', default = 2, help = 'Batch size per CPU/GPU for generation')\n",
    "\n",
    "# generation parameters\n",
    "parser.add_argument('--max_length', type = int, nargs = '?', default = 256, help = 'Max length of the prompt')\n",
    "parser.add_argument('--max_new_tokens', type = int, nargs = '?', default = 256, help = 'Max new tokens to generate')\n",
    "parser.add_argument('--do_sample', type = bool, nargs = '?', default = False, help = 'Whether to sample or not')\n",
    "parser.add_argument('--top_k', type = int, nargs = '?', default = 50, help = 'Top k tokens to sample from')\n",
    "parser.add_argument('--top_p', type = float, nargs = '?', default = 0.95, help = 'Top p tokens to sample from')\n",
    "\n",
    "args = parser.parse_args(args_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id: str = args.dataset_id\n",
    "split: str = args.split\n",
    "\n",
    "data_df: pd.DataFrame = datasets.load_dataset(dataset_id, split = split).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prompts: 100%|██████████| 4979/4979 [00:02<00:00, 1924.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed build_prompts in 2.619231700897217 seconds\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompts: List[Any] = calculate_time(build_prompts, data_df = data_df, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed build_tokenizer in 0.6552720069885254 seconds\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:00<00:00, 30.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed build_model in 65.6509382724762 seconds\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tokenizer: AutoTokenizer = calculate_time(build_tokenizer, args = args)\n",
    "model: AutoModelForCausalLM = calculate_time(build_model, args = args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_batched_inputs(args: argparse.Namespace, prompts: List[Any], tokenizer: AutoTokenizer) -> List[Any]:\n",
    "    inputs: List[Any] = tokenizer.batch_encode_plus(prompts, padding = True, truncation = True, return_tensors = 'pt').to('cpu')\n",
    "\n",
    "    return inputs\n",
    "\n",
    "def generate_from_prompts(args: argparse.Namespace, model: AutoModelForCausalLM, tokenizer: AutoTokenizer, prompts: List[Any], data_df: pd.DataFrame) -> List[Any]:\n",
    "    outputs: List[Any] = []\n",
    "\n",
    "    batch_size: int = args.batch_size\n",
    "    batch_size: int = 5\n",
    "    max_new_tokens: int = args.max_new_tokens\n",
    "    do_sample: bool = args.do_sample\n",
    "    top_k: int = args.top_k\n",
    "    top_p: float = args.top_p\n",
    "\n",
    "    def create_batches(prompts: List[Any], batch_size: int) -> List[List[Any]]:\n",
    "        batches: List[List[Any]] = []\n",
    "\n",
    "        for i in range(0, len(prompts), batch_size):\n",
    "            batches.append(prompts[i:i + batch_size])\n",
    "\n",
    "        return batches\n",
    "\n",
    "    batches: List[List[Any]] = create_batches(prompts, batch_size)\n",
    "\n",
    "    for batch in tqdm(batches, desc = 'Generating'):\n",
    "        # valid_inputs: List[Any] = build_inputs(batch, tokenizer)\n",
    "\n",
    "        # print(len(valid_inputs))\n",
    "        prompts = [sample['prompt'] for sample in batch]\n",
    "\n",
    "        inputs: List[Any] = build_batched_inputs(args = args, prompts = prompts, tokenizer = tokenizer)\n",
    "        inputs = inputs.to(model.device)\n",
    "        batch_outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens = max_new_tokens,\n",
    "            do_sample = do_sample,\n",
    "            top_k = top_k,\n",
    "            top_p = top_p,\n",
    "            eos_token_id = tokenizer.eos_token_id,\n",
    "            pad_token_id = tokenizer.pad_token_id,\n",
    "        )\n",
    "        inputs = inputs.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            truncated_outputs = [val[len(inputs[id]):] for id, val in enumerate(batch_outputs)]\n",
    "            batch_decoded_outputs = tokenizer.batch_decode(truncated_outputs, skip_special_tokens = True)\n",
    "\n",
    "            for i in range(len(batch)):\n",
    "                outputs.append(\n",
    "                    {\n",
    "                        'output': batch_decoded_outputs[i],\n",
    "                        'id': batch[i]['id'],\n",
    "                        'prompt': batch[i]['prompt'],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # save results every 5 batches\n",
    "        if (len(outputs) % 5 == 0):\n",
    "            results: List[Any] = decode_outputs(tokenizer = tokenizer, outputs = outputs)\n",
    "\n",
    "            save_results(args = args, results = results, data_df = data_df)\n",
    "\n",
    "            print(f'saved results for {len(outputs)} samples')\n",
    "            print('-' * 50)\n",
    "            print()\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\n",
      "### Instruction:\n",
      "rewrite below method from library \"org.springframework.security.oauth:spring-security-oauth2\" to \"io.jsonwebtoken:jjwt\". ONLY WRITE CODE, NO COMMENTS, IMPORTS, TEXT, NO EXPLAIN.\n",
      "```\n",
      "@Override\n",
      "    public boolean upgrade() {\n",
      "        logger.info(\"Deploying registered {} domain\", ADMIN_DOMAIN);\n",
      "        try {\n",
      "                               \n",
      "            Domain adminDomain = domainService.findById(ADMIN_DOMAIN).blockingGet();\n",
      "            eventManager.publishEvent(DomainEvent.DEPLOY, adminDomain);\n",
      "            return true;\n",
      "        } catch (DomainNotFoundException dnfe) {\n",
      "            logger.error(\"Failed to find admin domain\", dnfe);\n",
      "            throw new IllegalStateException(\"Failed to deploy admin domain\", dnfe);\n",
      "        }\n",
      "    }\n",
      "```\n",
      "\n",
      "### Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': prompts[0]['prompt']\n",
    "    },\n",
    "]\n",
    "\n",
    "vcl = tokenizer.apply_chat_template(messages, add_generation_prompt = True, padding = True, truncation = True, tokenize = False, return_tensors = 'pt')\n",
    "\n",
    "print(vcl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating:   0%|          | 0/996 [07:31<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed generate_from_prompts in 451.3585968017578 seconds\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs: List[Any] = calculate_time(generate_from_prompts, args = args, model = model, tokenizer = tokenizer, prompts = prompts, data_df = data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.split_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```java\n",
      "public void visit(OWLDataMinCardinality<?> ce) {\n",
      "    writeCardinality(1, ce);\n",
      "}\n",
      "```\n",
      "### Response:\n",
      "```java\n",
      "public void visit(OWLDataMinCardinality<?> ce) {\n",
      "    writeCardinality(ce.getFiller(), ce);\n",
      "}\n",
      "```\n",
      "\n",
      "### Response:\n",
      "```java\n",
      "public void visit(OWLDataMinCardinality<?> ce) {\n",
      "    writeCardinality(1, ce);\n",
      "}\n",
      "```\n",
      "\n",
      "### Response:\n",
      "```java\n",
      "public void visit(OWLDataMinCardinality<?> ce) {\n",
      "    writeCardinality(1, ce);\n",
      "}\n",
      "```\n",
      "\n",
      "### Response:\n",
      "```java\n",
      "public void visit(OWLDataMinCardinality<?> ce) {\n",
      "    writeCardinality(1, ce);\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "id = 2\n",
    "# print(outputs[id]['prompt'])\n",
    "# print('-' * 50)\n",
    "print(outputs[id]['output'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
