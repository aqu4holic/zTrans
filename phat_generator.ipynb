{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "seed = 18022004\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prefix: str = 'data'\n",
    "repo_prefix: str = f'{data_prefix}/repos'\n",
    "\n",
    "data_name = 'data_method_30k_test.parquet'\n",
    "\n",
    "data_df: pd.DataFrame = pd.read_parquet(f'{data_prefix}/{data_name}', engine = 'pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "hf_dataset = Dataset.from_pandas(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = hf_dataset.train_test_split(test_size = 0.2, seed = seed)\n",
    "\n",
    "val_test_split = train_test_split['test'].train_test_split(test_size = 0.5, seed = seed)\n",
    "\n",
    "final_datasets = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': val_test_split['train'],\n",
    "    'test': val_test_split['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'fromLib', 'toLib', 'repoName', 'prevCommit', 'startCommit', 'endCommit', 'fileName', 'startCommitChanges', 'endCommitChanges', 'startCode_cleaned', 'endCode_cleaned', 'diff_cleaned', 'methods_before', 'methods_after', 'methods_diff'],\n",
       "        num_rows: 23800\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'fromLib', 'toLib', 'repoName', 'prevCommit', 'startCommit', 'endCommit', 'fileName', 'startCommitChanges', 'endCommitChanges', 'startCode_cleaned', 'endCode_cleaned', 'diff_cleaned', 'methods_before', 'methods_after', 'methods_diff'],\n",
       "        num_rows: 2975\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'fromLib', 'toLib', 'repoName', 'prevCommit', 'startCommit', 'endCommit', 'fileName', 'startCommitChanges', 'endCommitChanges', 'startCode_cleaned', 'endCode_cleaned', 'diff_cleaned', 'methods_before', 'methods_after', 'methods_diff'],\n",
       "        num_rows: 2976\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:03<00:00,  1.23ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:02<00:00,  1.53ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:03<00:00,  1.30ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:02<00:00,  1.39ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:03<00:00,  1.31ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:02<00:00,  1.49ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 6/6 [01:37<00:00, 16.18s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:01<00:00,  1.62ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:09<00:00,  9.93s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:02<00:00,  1.43ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:11<00:00, 11.13s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/blackwhite1337/zTrans_dataset/commit/427e42bb6c28707798e4fdb7685a1a9f08eda5b5', commit_message='Upload dataset', commit_description='', oid='427e42bb6c28707798e4fdb7685a1a9f08eda5b5', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/blackwhite1337/zTrans_dataset', endpoint='https://huggingface.co', repo_type='dataset', repo_id='blackwhite1337/zTrans_dataset'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "access_token = os.environ.get('HF_TOKEN')\n",
    "\n",
    "login(token = access_token)\n",
    "\n",
    "username = 'blackwhite1337'\n",
    "dataset_name = 'zTrans_dataset'\n",
    "\n",
    "final_datasets.push_to_hub(f'{username}/{dataset_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 23800/23800 [00:12<00:00, 1890.35 examples/s]\n",
      "Generating validation split: 100%|██████████| 2975/2975 [00:03<00:00, 949.63 examples/s] \n",
      "Generating test split: 100%|██████████| 2976/2976 [00:04<00:00, 710.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "username = 'blackwhite1337'\n",
    "dataset_name = 'zTrans_dataset'\n",
    "\n",
    "dataset = load_dataset(f'{username}/{dataset_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format:   0%|          | 0/24 [00:00<?, ?ba/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 24/24 [00:10<00:00,  2.29ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:01<00:00,  2.42ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:01<00:00,  2.48ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "365139365"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset['train'].to_parquet(f'{data_prefix}/data_method_30k_train.parquet')\n",
    "dataset['validation'].to_parquet(f'{data_prefix}/data_method_30k_val.parquet')\n",
    "dataset['test'].to_parquet(f'{data_prefix}/data_method_30k_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = final_datasets['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts: List[str] = []\n",
    "\n",
    "BEGIN_TOKEN: str = '<｜fim▁begin｜>'\n",
    "FILL_TOKEN: str = '<｜fim▁hole｜>'\n",
    "END_TOKEN: str = '<｜fim▁end｜>'\n",
    "\n",
    "prompt_template: str = '''// rewrite below method from library \"{}\" to \"{}\"\n",
    "// ONLY write method code with no comments, imports. DONT WRITE TEXT.\n",
    "{}\n",
    "'''\n",
    "\n",
    "for id in range(len(data_df)):\n",
    "    line = data_df.iloc[id]\n",
    "\n",
    "    from_lib: str = line['fromLib']\n",
    "    to_lib: str = line['toLib']\n",
    "    method_before: str = line['methods_before']\n",
    "    file_name = line['fileName']\n",
    "\n",
    "    if (len(method_before) == 0):\n",
    "        continue\n",
    "\n",
    "    prompt: str = prompt_template.format(from_lib, to_lib, method_before)\n",
    "    ground_truth: str = line['methods_after']\n",
    "\n",
    "    prompts.append({'prompt': prompt, 'ground_truth': ground_truth})\n",
    "\n",
    "    if (len(prompts) == 2):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// rewrite below method from library \"net.sf.ehcache:ehcache\" to \"org.ehcache:ehcache\"\n",
      "// ONLY write ```method code``` with no comments, imports. DONT WRITE TEXT.\n",
      "public void clear(final String context) {\n",
      "\t\tfinal Ehcache ehCache = manager.getCache(context);\n",
      "\t\tif (ehCache != null) {\n",
      "\t\t\tehCache.removeAll();\n",
      "\t\t}\n",
      "\t}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompts[1]['prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/drive2/phatnt/zTrans/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:28<00:00, 14.43s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_id: str = 'deepseek-ai/deepseek-coder-6.7b-instruct'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code = True,)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit = True,\n",
    ")\n",
    "\n",
    "device: str = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code = True,\n",
    "    quantization_config = quantization_config,\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map = 'auto',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/drive2/phatnt/zTrans/venv/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/drive2/phatnt/zTrans/venv/lib/python3.10/site-packages/transformers/generation/utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# inputs = tokenizer.encode(prompt, return_tensors = 'pt').to(model.device)\n",
    "\n",
    "messages = []\n",
    "\n",
    "for i in range(len(prompts)):\n",
    "    messages.append({\n",
    "        'role': 'user',\n",
    "        'content': prompts[i]['prompt'],\n",
    "    },)\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt = True, return_tensors = 'pt').to('cpu')\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens = 256,\n",
    "    do_sample = False,\n",
    "    top_k = 50,\n",
    "    top_p = 0.95,\n",
    "    eos_token_id = tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "628"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 2\u001b[0m     output_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;28mlen\u001b[39m(inputs[\u001b[38;5;241m1\u001b[39m]):], skip_special_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_text)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output_text = tokenizer.decode(outputs[1][len(inputs[1]):], skip_special_tokens = True,)\n",
    "\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.llama.modeling_llama.LlamaForCausalLM"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public void contextInitialized(ServletContextEvent event) {\n",
      "        ServletContext context = event.getServletContext();\n",
      "        SimpleClassScanner scanner = SimpleClassScanner.getInstance();\n",
      "        Set<String> packages = scanner.getPackages(true);\n",
      "        Object sc = context.getAttribute(\"javax.websocket.server.ServerContainer\");\n",
      "        if (sc instanceof ServerContainer) {\n",
      "            ServerContainer container = (ServerContainer) sc;\n",
      "            int total = 0;\n",
      "            for (String p : packages) {\n",
      "                List<Class<?>> endpoints = scanner.getAnnotatedClasses(p, ServerEndpoint.class);\n",
      "                for (Class<?> cls : endpoints) {\n",
      "                    if (!Feature.isRequired(cls)) {\n",
      "                        continue;\n",
      "                    }\n",
      "                    try {\n",
      "                        container.addEndpoint(cls);\n",
      "                        ServerEndpoint ep = cls.getAnnotation(ServerEndpoint.class);\n",
      "                        total++;\n",
      "                        log.info(\"{} registered as WEBSOCKET DISPATCHER {}\", cls.getName(), Arrays.asList(ep.value()));\n",
      "                    } catch (DeploymentException e) {\n",
      "                        log.error(\"Unable to deploy websocket endpoint {} - {}\", cls, e.getMessage());\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "            if (total > 0) {\n",
      "                log.info(\"Total {} Websocket server endpoint{} registered (JSR-356)\", total, total == 1 ? \" is\" : \"s are\");\n",
      "            }\n",
      "        } else {\n",
      "            log.error(\"Unable to register any ServerEndpoints because javax.websocket.server.ServerContainer is not available\");\n",
      "        }\n",
      "    }\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0]['ground_truth'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "diff = difflib.unified_diff(prompt.splitlines(), output_text.splitlines(), lineterm = '')\n",
    "\n",
    "print('\\n'.join(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
