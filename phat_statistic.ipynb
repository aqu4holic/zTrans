{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prefix: str = 'data'\n",
    "\n",
    "test_data = pd.read_csv(f'{data_prefix}/500_test_sample_dataset.csv')\n",
    "log_data = pd.read_csv(f'{data_prefix}/500_log_sample_dataset.csv')\n",
    "all_data = pd.read_csv(f'{data_prefix}/2000_code_samples_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "others_data =  pd.read_parquet(f'{data_prefix}/500_others_sample_dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_diff(diff: str):\n",
    "    changes = []\n",
    "\n",
    "    # Initialize counters for current section\n",
    "    current_added_count = 0\n",
    "    current_removed_count = 0\n",
    "    section_identifier = None  # To store the section from the @@ line\n",
    "    section_count = 0  # To keep track of the number of @@ sections\n",
    "    lib_changes_count = 0\n",
    "\n",
    "    for line in diff.splitlines():\n",
    "        if line.startswith('@@'):\n",
    "            # If we reach a new change section, append previous counts if any\n",
    "            if current_added_count > 0 or current_removed_count > 0:\n",
    "                changes.append({\n",
    "                    'section_count': section_count,\n",
    "                    'section_identifier': section_identifier,\n",
    "                    'added_count': current_added_count,\n",
    "                    'removed_count': current_removed_count,\n",
    "                    'line_change': current_added_count - current_removed_count,\n",
    "                    'lib_changes': lib_changes_count\n",
    "                })\n",
    "                # Reset counts for the new section\n",
    "                current_added_count = 0\n",
    "                current_removed_count = 0\n",
    "                lib_changes_count = 0\n",
    "\n",
    "            # Extract the old and new line numbers from the @@ line\n",
    "            parts = line.split()\n",
    "            old_line_info = parts[1]  # e.g., -16,14\n",
    "            new_line_info = parts[2]  # e.g., +16,14\n",
    "\n",
    "            # Get the old and new line numbers\n",
    "            old_start = int(old_line_info.split(',')[0][1:])  # Skip the '-'\n",
    "            new_start = int(new_line_info.split(',')[0][1:])  # Skip the '+'\n",
    "\n",
    "            # Store the start line numbers\n",
    "            section_identifier = {\n",
    "                'old_start': old_start,\n",
    "                'new_start': new_start\n",
    "            }\n",
    "            section_count += 1\n",
    "\n",
    "        elif line.startswith('+'):\n",
    "            # Count added lines\n",
    "            current_added_count += 1\n",
    "            if \"import\" in line:\n",
    "                lib_changes_count += 1\n",
    "\n",
    "        elif line.startswith('-'):\n",
    "            # Count removed lines\n",
    "            current_removed_count += 1\n",
    "            if \"import\" in line:\n",
    "                lib_changes_count += 1\n",
    "\n",
    "    # Append any remaining counts after the last section\n",
    "    if current_added_count > 0 or current_removed_count > 0:\n",
    "        changes.append({\n",
    "            'section_count': section_count,\n",
    "            'section_identifier': section_identifier,\n",
    "            'added_count': current_added_count,\n",
    "            'removed_count': current_removed_count,\n",
    "            'line_change': current_added_count - current_removed_count,\n",
    "            'lib_changes': lib_changes_count\n",
    "        })\n",
    "\n",
    "    return changes\n",
    "\n",
    "\n",
    "def statistic_dataframe(df):\n",
    "    # return number of file based on startCommit, endCommit and sort on repoName (same repoName near each other)\n",
    "    unique_commit_repo_mapping = (\n",
    "        df.groupby(['startCommit', 'endCommit'])\n",
    "        .agg(repoName=('repoName', 'first'), count=('repoName', 'size'))\n",
    "        .reset_index()\n",
    "        .sort_values(by='repoName')\n",
    "    )\n",
    "\n",
    "    # return repos that have more than 1 startCommit and endCommit changes\n",
    "    repoName_counts = unique_commit_repo_mapping['repoName'].value_counts()\n",
    "    repos_more_than_once_counts = repoName_counts[repoName_counts > 1]\n",
    "\n",
    "    # file created after commit\n",
    "    new_file_df = df[\n",
    "        (pd.isna(df['startCode'])) & (pd.notna(df['endCode']))\n",
    "    ]\n",
    "\n",
    "    # file deleted after commit\n",
    "    deleted_df = df[\n",
    "        (pd.notna(df['startCode'])) & (pd.isna(df['endCode']))\n",
    "    ]\n",
    "\n",
    "    # diff analyze\n",
    "    remain_df = df.merge(new_file_df, how='outer', on=df.columns.tolist(), indicator=True)\n",
    "    remain_df = remain_df[remain_df['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "    remain_df = remain_df.merge(deleted_df, how='outer', on=df.columns.tolist(), indicator=True)\n",
    "    remain_df = remain_df[remain_df['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "    remain_df['total_added'] = 0\n",
    "    remain_df['total_removed'] = 0\n",
    "    remain_df['total_position'] = 0\n",
    "    remain_df['detailed_changes'] = None\n",
    "    for index, row in remain_df.iterrows():\n",
    "        total_added = 0\n",
    "        total_removed = 0\n",
    "        total_position = 0\n",
    "        total_lib_change = 0\n",
    "        diff = row['diff']\n",
    "        changes = process_diff(diff)\n",
    "        total_position = len(changes)\n",
    "        for change in changes:\n",
    "            if change['section_identifier'] != None:\n",
    "                total_added += change['added_count']\n",
    "                total_removed += change['removed_count']\n",
    "                total_lib_change += change['lib_changes']\n",
    "        remain_df.at[index, 'total_added'] = total_added\n",
    "        remain_df.at[index, 'total_removed'] = total_removed\n",
    "        remain_df.at[index, 'total_position'] = total_position\n",
    "        remain_df.at[index, 'detailed_changes'] = changes\n",
    "        if (total_added + total_removed == 0):\n",
    "            remain_df.at[index, 'lib_percentage'] = 0\n",
    "        else:\n",
    "            remain_df.at[index, 'lib_percentage'] = total_lib_change / (total_added + total_removed)\n",
    "\n",
    "    # library analyze\n",
    "    unique_lib_mapping = (\n",
    "        df.groupby(['fromLib', 'toLib'])\n",
    "        .size()  # This counts the number of occurrences for each combination\n",
    "        .reset_index(name='count')  # Reset index and name the count column\n",
    "        .sort_values(by='count', ascending=False)  # Sort by count in descending order\n",
    "    )\n",
    "\n",
    "    # Return results\n",
    "    return {\n",
    "        'unique_commit_repo_mapping': unique_commit_repo_mapping,\n",
    "        'repos_more_than_once_counts': repos_more_than_once_counts,\n",
    "        'new_file_count': new_file_df,\n",
    "        'deleted_file_count': deleted_df,\n",
    "        'remain_df': remain_df,\n",
    "        'unique_lib_mapping': unique_lib_mapping\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'unique'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3371064/2445373568.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatistic_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# pd.set_option('display.max_row', None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'new_file_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'repoName'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/drive2/phatnt/zTrans/venv/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6295\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6296\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'unique'"
     ]
    }
   ],
   "source": [
    "result = statistic_dataframe(test_data)\n",
    "# pd.set_option('display.max_row', None)\n",
    "result['new_file_count']['repoName'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
