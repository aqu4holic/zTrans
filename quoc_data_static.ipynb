{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_parquet('/drive2/phatnt/zTrans/data/data_method_tresitter1.parquet',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fromLib                                                           ant:ant\n",
      "toLib                                                  org.apache.ant:ant\n",
      "repoName                                              bobmcwhirter_drools\n",
      "prevCommit                       2ba5d35fb486c4a16b5b8b15fc247e8759bfa54f\n",
      "startCommit                      92384035a8651b675c82689b24837eb8adb81d66\n",
      "endCommit                        0df25aead759b3b542a685ac21a3a009bcd22fe9\n",
      "fileName                drools-compiler/src/test/java/org/drools/guvno...\n",
      "startCode               package org.drools.guvnor.server.util;\\n\\nimpo...\n",
      "endCode                 package org.drools.guvnor.server.util;\\n\\nimpo...\n",
      "startCommitChanges                               +org.mockito:mockito-all\n",
      "endCommitChanges                -org.jmock:jmock\\n-org.jmock:jmock-legacy\n",
      "startCode_cleaned       package org.drools.guvnor.server.util;\\nimport...\n",
      "endCode_cleaned         package org.drools.guvnor.server.util;\\nimport...\n",
      "diff_cleaned            --- \\n+++ \\n@@ -3,11 +3,9 @@\\n import org.droo...\n",
      "total_methods_before                                                    4\n",
      "total_methods_after                                                     4\n",
      "methods_before          public void testAddSentenceMultipleTypes() {\\n...\n",
      "methods_after           public void testAddSentenceMultipleTypes() {\\n...\n",
      "methods_name                                 testAddSentenceMultipleTypes\n",
      "method_signature                   public  testAddSentenceMultipleTypes()\n",
      "remove_line                                                            15\n",
      "add_line                                                               12\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co : 52.99747311368547 % la special\n"
     ]
    }
   ],
   "source": [
    "data_before_null = data[data['methods_before'] == '']\n",
    "data_after_null = data[data['methods_after'] == '']\n",
    "special = pd.concat([data_before_null, data_after_null], ignore_index=True)\n",
    "print('Co :',len(special) *100 / len(data) , '% la special')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "def get_diff_lines(method_1,method_2):\n",
    "    diff = difflib.unified_diff(\n",
    "    method_1.splitlines(), \n",
    "    method_2.splitlines(), \n",
    "    lineterm='',  # Avoid newline being added to the output\n",
    "    fromfile='Method 1',\n",
    "    tofile='Method 2'\n",
    "    )\n",
    "    diff = '\\n'.join(diff)\n",
    "    diff_changes = diff\n",
    "    diff = diff.splitlines()\n",
    "    \n",
    "    remove = 0\n",
    "    add = 0\n",
    "    for i in diff:\n",
    "        if i.startswith('-') and '---' not in i:\n",
    "            remove+=1\n",
    "        elif i.startswith('+') and '+++' not in i:\n",
    "            add += 1\n",
    "    return {'remove' : remove,'add' : add,'diff_changes' : diff_changes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['remove_line', 'add_line','diff_changes']] = data.apply(\n",
    "    lambda row: pd.Series(get_diff_lines(row['methods_before'], row['methods_after'])),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data.to_parquet('/drive2/phatnt/zTrans/data/data_method_tresitter1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loại bỏ các special\n",
    "rows_to_remove = pd.concat([data_before_null, data_after_null])\n",
    "\n",
    "data = data[~data.index.isin(rows_to_remove.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>startCommit</th>\n",
       "      <th>endCommit</th>\n",
       "      <th>total_count</th>\n",
       "      <th>special_count</th>\n",
       "      <th>special_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00e9a259ab6d8bde0456bdf168fed66a7cedbcff</td>\n",
       "      <td>00e9a259ab6d8bde0456bdf168fed66a7cedbcff</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0188e9aa280b800710848d68a93af4cb28b050da</td>\n",
       "      <td>0188e9aa280b800710848d68a93af4cb28b050da</td>\n",
       "      <td>289</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.318339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>019d1e6e6e3715b2be91b01a48a3bd4d14a5b216</td>\n",
       "      <td>019d1e6e6e3715b2be91b01a48a3bd4d14a5b216</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01ae58600c4069cd4df5081d52e81e05411694d9</td>\n",
       "      <td>01ae58600c4069cd4df5081d52e81e05411694d9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01ca345463fdaee4d5d72293bb7679fb5bc815ec</td>\n",
       "      <td>01ca345463fdaee4d5d72293bb7679fb5bc815ec</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>fe5c7d3f17529442f7a62355a92d51a294ad2769</td>\n",
       "      <td>fe5c7d3f17529442f7a62355a92d51a294ad2769</td>\n",
       "      <td>99</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.282828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>fee22c96dcbf09bf8d0ea5dff58a9191e04b4398</td>\n",
       "      <td>fee22c96dcbf09bf8d0ea5dff58a9191e04b4398</td>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>ff048221f4cbcbc885343461a2572404c6020753</td>\n",
       "      <td>ff048221f4cbcbc885343461a2572404c6020753</td>\n",
       "      <td>282</td>\n",
       "      <td>142.0</td>\n",
       "      <td>0.503546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>ff661a403578c638852652ee93a5e47f447903f9</td>\n",
       "      <td>ff661a403578c638852652ee93a5e47f447903f9</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>ff848386f71e5c435860f9cb997cce30d492d161</td>\n",
       "      <td>ff848386f71e5c435860f9cb997cce30d492d161</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>874 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  startCommit  \\\n",
       "0    00e9a259ab6d8bde0456bdf168fed66a7cedbcff   \n",
       "1    0188e9aa280b800710848d68a93af4cb28b050da   \n",
       "2    019d1e6e6e3715b2be91b01a48a3bd4d14a5b216   \n",
       "3    01ae58600c4069cd4df5081d52e81e05411694d9   \n",
       "4    01ca345463fdaee4d5d72293bb7679fb5bc815ec   \n",
       "..                                        ...   \n",
       "869  fe5c7d3f17529442f7a62355a92d51a294ad2769   \n",
       "870  fee22c96dcbf09bf8d0ea5dff58a9191e04b4398   \n",
       "871  ff048221f4cbcbc885343461a2572404c6020753   \n",
       "872  ff661a403578c638852652ee93a5e47f447903f9   \n",
       "873  ff848386f71e5c435860f9cb997cce30d492d161   \n",
       "\n",
       "                                    endCommit  total_count  special_count  \\\n",
       "0    00e9a259ab6d8bde0456bdf168fed66a7cedbcff            2            0.0   \n",
       "1    0188e9aa280b800710848d68a93af4cb28b050da          289           92.0   \n",
       "2    019d1e6e6e3715b2be91b01a48a3bd4d14a5b216            2            0.0   \n",
       "3    01ae58600c4069cd4df5081d52e81e05411694d9            3            0.0   \n",
       "4    01ca345463fdaee4d5d72293bb7679fb5bc815ec            1            1.0   \n",
       "..                                        ...          ...            ...   \n",
       "869  fe5c7d3f17529442f7a62355a92d51a294ad2769           99           28.0   \n",
       "870  fee22c96dcbf09bf8d0ea5dff58a9191e04b4398            8            4.0   \n",
       "871  ff048221f4cbcbc885343461a2572404c6020753          282          142.0   \n",
       "872  ff661a403578c638852652ee93a5e47f447903f9            3            0.0   \n",
       "873  ff848386f71e5c435860f9cb997cce30d492d161            2            0.0   \n",
       "\n",
       "     special_ratio  \n",
       "0         0.000000  \n",
       "1         0.318339  \n",
       "2         0.000000  \n",
       "3         0.000000  \n",
       "4         1.000000  \n",
       "..             ...  \n",
       "869       0.282828  \n",
       "870       0.500000  \n",
       "871       0.503546  \n",
       "872       0.000000  \n",
       "873       0.000000  \n",
       "\n",
       "[874 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special = data[(data['methods_before'] == '') | (data['methods_after'] == '')]\n",
    "total_counts = data.groupby([\"startCommit\", \"endCommit\"]).size().reset_index(name=\"total_count\")\n",
    "special_counts = special.groupby([\"startCommit\", \"endCommit\"]).size().reset_index(name=\"special_count\")\n",
    "summary = pd.merge(total_counts, special_counts, on=[\"startCommit\", \"endCommit\"], how=\"left\").fillna(0)\n",
    "summary[\"special_ratio\"] = summary[\"special_count\"] / summary[\"total_count\"]\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count      874.000000\n",
       "mean        49.673913\n",
       "std        421.569098\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          3.000000\n",
       "75%         13.000000\n",
       "max      10373.000000\n",
       "Name: special_count, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tbinh có khoảng 50 methods special\n",
    "summary['special_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    874.000000\n",
       "mean       0.318187\n",
       "std        0.321754\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.241212\n",
       "75%        0.565600\n",
       "max        1.000000\n",
       "Name: special_ratio, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tbinh trong 1 migration thì có 31% là special\n",
    "summary['special_ratio'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    15490.000000\n",
       "mean         5.288509\n",
       "std         11.751422\n",
       "min          1.000000\n",
       "25%          1.000000\n",
       "50%          2.000000\n",
       "75%          5.000000\n",
       "max        728.000000\n",
       "Name: methods_before_count, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 class có tầm 75% là thay đổi dưới 5 methods, trung bình thay đổi 5 methods\n",
    "method_counts = data.groupby(\"startCode\").size().reset_index(name=\"methods_before_count\")\n",
    "method_counts['methods_before_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    81919.000000\n",
       "mean        10.324369\n",
       "std         15.041808\n",
       "min          0.000000\n",
       "25%          3.000000\n",
       "50%          6.000000\n",
       "75%         12.000000\n",
       "max        522.000000\n",
       "Name: changes_line, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tbinh 1 method thay đôi khoảng 10 dòng\n",
    "data['changes_line'] = data.apply(lambda x: x['remove_line'] + x['add_line'], axis=1)\n",
    "data['changes_line'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@SuppressWarnings({\"fallthrough\", \"nls\"})\n",
      "  public boolean doPhase1(ASTNode ast, QB qb, Phase1Ctx ctx_1, PlannerContext plannerCtx)\n",
      "      throws SemanticException {\n",
      "    boolean phase1Result = true;\n",
      "    QBParseInfo qbp = qb.getParseInfo();\n",
      "    boolean skipRecursion = false;\n",
      "    if (ast.getToken() != null) {\n",
      "      skipRecursion = true;\n",
      "      switch (ast.getToken().getType()) {\n",
      "      case HiveParser.TOK_SELECTDI:\n",
      "        qb.countSelDi();\n",
      "      case HiveParser.TOK_SELECT:\n",
      "        qb.countSel();\n",
      "        qbp.setSelExprForClause(ctx_1.dest, ast);\n",
      "        int posn = 0;\n",
      "        if (((ASTNode) ast.getChild(0)).getToken().getType() == HiveParser.TOK_HINTLIST) {\n",
      "          qbp.setHints((ASTNode) ast.getChild(0));\n",
      "          posn++;\n",
      "        }\n",
      "        if ((ast.getChild(posn).getChild(0).getType() == HiveParser.TOK_TRANSFORM))\n",
      "          queryProperties.setUsesScript(true);\n",
      "        LinkedHashMap<String, ASTNode> aggregations = doPhase1GetAggregationsFromSelect(ast,\n",
      "            qb, ctx_1.dest);\n",
      "        doPhase1GetColumnAliasesFromSelect(ast, qbp);\n",
      "        qbp.setAggregationExprsForClause(ctx_1.dest, aggregations);\n",
      "        qbp.setDistinctFuncExprsForClause(ctx_1.dest,\n",
      "          doPhase1GetDistinctFuncExprs(aggregations));\n",
      "        break;\n",
      "      case HiveParser.TOK_WHERE:\n",
      "        qbp.setWhrExprForClause(ctx_1.dest, ast);\n",
      "        if (!SubQueryUtils.findSubQueries((ASTNode) ast.getChild(0)).isEmpty())\n",
      "            queryProperties.setFilterWithSubQuery(true);\n",
      "        break;\n",
      "      case HiveParser.TOK_INSERT_INTO:\n",
      "        String currentDatabase = SessionState.get().getCurrentDatabase();\n",
      "        String tab_name = getUnescapedName((ASTNode) ast.getChild(0).getChild(0), currentDatabase);\n",
      "        qbp.addInsertIntoTable(tab_name, ast);\n",
      "      case HiveParser.TOK_DESTINATION:\n",
      "        ctx_1.dest = this.ctx.getDestNamePrefix(ast).toString() + ctx_1.nextNum;\n",
      "        ctx_1.nextNum++;\n",
      "        boolean isTmpFileDest = false;\n",
      "        if (ast.getChildCount() > 0 && ast.getChild(0) instanceof ASTNode) {\n",
      "          ASTNode ch = (ASTNode) ast.getChild(0);\n",
      "          if (ch.getToken().getType() == HiveParser.TOK_DIR && ch.getChildCount() > 0\n",
      "              && ch.getChild(0) instanceof ASTNode) {\n",
      "            ch = (ASTNode) ch.getChild(0);\n",
      "            isTmpFileDest = ch.getToken().getType() == HiveParser.TOK_TMP_FILE;\n",
      "          } else {\n",
      "            if (ast.getToken().getType() == HiveParser.TOK_DESTINATION\n",
      "                && ast.getChild(0).getType() == HiveParser.TOK_TAB) {\n",
      "              String fullTableName = getUnescapedName((ASTNode) ast.getChild(0).getChild(0),\n",
      "                  SessionState.get().getCurrentDatabase());\n",
      "              qbp.getInsertOverwriteTables().put(fullTableName, ast);\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "        if (qbp.getIsSubQ() && !isTmpFileDest) {\n",
      "          throw new SemanticException(ErrorMsg.NO_INSERT_INSUBQUERY.getMsg(ast));\n",
      "        }\n",
      "        if (plannerCtx != null) {\n",
      "          plannerCtx.setInsertToken(ast, isTmpFileDest);\n",
      "        }\n",
      "        qbp.setDestForClause(ctx_1.dest, (ASTNode) ast.getChild(0));\n",
      "        handleInsertStatementSpecPhase1(ast, qbp, ctx_1);\n",
      "        if (qbp.getClauseNamesForDest().size() > 1) {\n",
      "          queryProperties.setMultiDestQuery(true);\n",
      "        }\n",
      "        break;\n",
      "      case HiveParser.TOK_FROM:\n",
      "        int child_count = ast.getChildCount();\n",
      "        if (child_count != 1) {\n",
      "          throw new SemanticException(generateErrorMessage(ast,\n",
      "              \"Multiple Children \" + child_count));\n",
      "        }\n",
      "        ASTNode frm = (ASTNode) ast.getChild(0);\n",
      "        if (frm.getToken().getType() == HiveParser.TOK_TABREF) {\n",
      "          processTable(qb, frm);\n",
      "        } else if (frm.getToken().getType() == HiveParser.TOK_VIRTUAL_TABLE) {\n",
      "          ASTNode newFrom = genValuesTempTable(frm, qb);\n",
      "          ast.setChild(0, newFrom);\n",
      "          processTable(qb, newFrom);\n",
      "        } else if (frm.getToken().getType() == HiveParser.TOK_SUBQUERY) {\n",
      "          processSubQuery(qb, frm);\n",
      "        } else if (frm.getToken().getType() == HiveParser.TOK_LATERAL_VIEW ||\n",
      "            frm.getToken().getType() == HiveParser.TOK_LATERAL_VIEW_OUTER) {\n",
      "          queryProperties.setHasLateralViews(true);\n",
      "          processLateralView(qb, frm);\n",
      "        } else if (isJoinToken(frm)) {\n",
      "          processJoin(qb, frm);\n",
      "          qbp.setJoinExpr(frm);\n",
      "        }else if(frm.getToken().getType() == HiveParser.TOK_PTBLFUNCTION){\n",
      "          queryProperties.setHasPTF(true);\n",
      "          processPTF(qb, frm);\n",
      "        }\n",
      "        break;\n",
      "      case HiveParser.TOK_CLUSTERBY:\n",
      "        queryProperties.setHasClusterBy(true);\n",
      "        qbp.setClusterByExprForClause(ctx_1.dest, ast);\n",
      "        break;\n",
      "      case HiveParser.TOK_DISTRIBUTEBY:\n",
      "        queryProperties.setHasDistributeBy(true);\n",
      "        qbp.setDistributeByExprForClause(ctx_1.dest, ast);\n",
      "        if (qbp.getClusterByForClause(ctx_1.dest) != null) {\n",
      "          throw new SemanticException(generateErrorMessage(ast,\n",
      "              ErrorMsg.CLUSTERBY_DISTRIBUTEBY_CONFLICT.getMsg()));\n",
      "        } else if (qbp.getOrderByForClause(ctx_1.dest) != null) {\n",
      "          throw new SemanticException(generateErrorMessage(ast,\n",
      "              ErrorMsg.ORDERBY_DISTRIBUTEBY_CONFLICT.getMsg()));\n",
      "        }\n",
      "        break;\n",
      "      case HiveParser.TOK_SORTBY:\n",
      "        queryProperties.setHasSortBy(true);\n",
      "        qbp.setSortByExprForClause(ctx_1.dest, ast);\n",
      "        if (qbp.getClusterByForClause(ctx_1.dest) != null) {\n",
      "          throw new SemanticException(generateErrorMessage(ast,\n",
      "              ErrorMsg.CLUSTERBY_SORTBY_CONFLICT.getMsg()));\n",
      "        } else if (qbp.getOrderByForClause(ctx_1.dest) != null) {\n",
      "          throw new SemanticException(generateErrorMessage(ast,\n",
      "              ErrorMsg.ORDERBY_SORTBY_CONFLICT.getMsg()));\n",
      "        }\n",
      "        break;\n",
      "      case HiveParser.TOK_ORDERBY:\n",
      "        queryProperties.setHasOrderBy(true);\n",
      "        qbp.setOrderByExprForClause(ctx_1.dest, ast);\n",
      "        if (qbp.getClusterByForClause(ctx_1.dest) != null) {\n",
      "          throw new SemanticException(generateErrorMessage(ast,\n",
      "              ErrorMsg.CLUSTERBY_ORDERBY_CONFLICT.getMsg()));\n",
      "        }\n",
      "        break;\n",
      "      case HiveParser.TOK_GROUPBY:\n",
      "      case HiveParser.TOK_ROLLUP_GROUPBY:\n",
      "      case HiveParser.TOK_CUBE_GROUPBY:\n",
      "      case HiveParser.TOK_GROUPING_SETS:\n",
      "        queryProperties.setHasGroupBy(true);\n",
      "        if (qbp.getJoinExpr() != null) {\n",
      "          queryProperties.setHasJoinFollowedByGroupBy(true);\n",
      "        }\n",
      "        if (qbp.getSelForClause(ctx_1.dest).getToken().getType() == HiveParser.TOK_SELECTDI) {\n",
      "          throw new SemanticException(generateErrorMessage(ast,\n",
      "              ErrorMsg.SELECT_DISTINCT_WITH_GROUPBY.getMsg()));\n",
      "        }\n",
      "        qbp.setGroupByExprForClause(ctx_1.dest, ast);\n",
      "        skipRecursion = true;\n",
      "        if (ast.getToken().getType() == HiveParser.TOK_ROLLUP_GROUPBY) {\n",
      "          qbp.getDestRollups().add(ctx_1.dest);\n",
      "        } else if (ast.getToken().getType() == HiveParser.TOK_CUBE_GROUPBY) {\n",
      "          qbp.getDestCubes().add(ctx_1.dest);\n",
      "        } else if (ast.getToken().getType() == HiveParser.TOK_GROUPING_SETS) {\n",
      "          qbp.getDestGroupingSets().add(ctx_1.dest);\n",
      "        }\n",
      "        break;\n",
      "      case HiveParser.TOK_HAVING:\n",
      "        qbp.setHavingExprForClause(ctx_1.dest, ast);\n",
      "        qbp.addAggregationExprsForClause(ctx_1.dest,\n",
      "            doPhase1GetAggregationsFromSelect(ast, qb, ctx_1.dest));\n",
      "        break;\n",
      "      case HiveParser.KW_WINDOW:\n",
      "        if (!qb.hasWindowingSpec(ctx_1.dest) ) {\n",
      "          throw new SemanticException(generateErrorMessage(ast,\n",
      "              \"Query has no Cluster/Distribute By; but has a Window definition\"));\n",
      "        }\n",
      "        handleQueryWindowClauses(qb, ctx_1, ast);\n",
      "        break;\n",
      "      case HiveParser.TOK_LIMIT:\n",
      "        if (ast.getChildCount() == 2) {\n",
      "          qbp.setDestLimit(ctx_1.dest,\n",
      "              new Integer(ast.getChild(0).getText()),\n",
      "              new Integer(ast.getChild(1).getText()));\n",
      "        } else {\n",
      "          qbp.setDestLimit(ctx_1.dest, new Integer(0),\n",
      "              new Integer(ast.getChild(0).getText()));\n",
      "        }\n",
      "        break;\n",
      "      case HiveParser.TOK_ANALYZE:\n",
      "        String table_name = getUnescapedName((ASTNode) ast.getChild(0).getChild(0)).toLowerCase();\n",
      "        qb.setTabAlias(table_name, table_name);\n",
      "        qb.addAlias(table_name);\n",
      "        qb.getParseInfo().setIsAnalyzeCommand(true);\n",
      "        qb.getParseInfo().setNoScanAnalyzeCommand(this.noscan);\n",
      "        qb.getParseInfo().setPartialScanAnalyzeCommand(this.partialscan);\n",
      "        HiveConf.setVar(conf, HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, \"nonstrict\");\n",
      "        HiveConf.setVar(conf, HiveConf.ConfVars.HIVEMAPREDMODE, \"nonstrict\");\n",
      "        break;\n",
      "      case HiveParser.TOK_UNIONALL:\n",
      "        if (!qbp.getIsSubQ()) {\n",
      "          throw new SemanticException(generateErrorMessage(ast,\n",
      "              ErrorMsg.UNION_NOTIN_SUBQ.getMsg()));\n",
      "        }\n",
      "        skipRecursion = false;\n",
      "        break;\n",
      "      case HiveParser.TOK_INSERT:\n",
      "        ASTNode destination = (ASTNode) ast.getChild(0);\n",
      "        Tree tab = destination.getChild(0);\n",
      "        if (destination.getChildCount() == 2 &&\n",
      "            tab.getChildCount() == 2 &&\n",
      "            destination.getChild(1).getType() == HiveParser.TOK_IFNOTEXISTS) {\n",
      "          String tableName = tab.getChild(0).getChild(0).getText();\n",
      "          Tree partitions = tab.getChild(1);\n",
      "          int childCount = partitions.getChildCount();\n",
      "          HashMap<String, String> partition = new HashMap<String, String>();\n",
      "          for (int i = 0; i < childCount; i++) {\n",
      "            String partitionName = partitions.getChild(i).getChild(0).getText();\n",
      "            Tree pvalue = partitions.getChild(i).getChild(1);\n",
      "            if (pvalue == null) {\n",
      "              break;\n",
      "            }\n",
      "            String partitionVal = stripQuotes(pvalue.getText());\n",
      "            partition.put(partitionName, partitionVal);\n",
      "          }\n",
      "          if (childCount != partition.size()) {\n",
      "            throw new SemanticException(ErrorMsg.INSERT_INTO_DYNAMICPARTITION_IFNOTEXISTS\n",
      "                .getMsg(partition.toString()));\n",
      "          }\n",
      "          Table table = null;\n",
      "          try {\n",
      "            table = this.getTableObjectByName(tableName);\n",
      "          } catch (HiveException ex) {\n",
      "            throw new SemanticException(ex);\n",
      "          }\n",
      "          try {\n",
      "            Partition parMetaData = db.getPartition(table, partition, false);\n",
      "            if (parMetaData != null) {\n",
      "              phase1Result = false;\n",
      "              skipRecursion = true;\n",
      "              LOG.info(\"Partition already exists so insert into overwrite \" +\n",
      "                  \"skipped for partition : \" + parMetaData.toString());\n",
      "              break;\n",
      "            }\n",
      "          } catch (HiveException e) {\n",
      "            LOG.info(\"Error while getting metadata : \", e);\n",
      "          }\n",
      "          validatePartSpec(table, partition, (ASTNode)tab, conf, false);\n",
      "        }\n",
      "        skipRecursion = false;\n",
      "        break;\n",
      "      case HiveParser.TOK_LATERAL_VIEW:\n",
      "      case HiveParser.TOK_LATERAL_VIEW_OUTER:\n",
      "        assert ast.getChildCount() == 1;\n",
      "        qb.getParseInfo().getDestToLateralView().put(ctx_1.dest, ast);\n",
      "        break;\n",
      "      case HiveParser.TOK_CTE:\n",
      "        processCTE(qb, ast);\n",
      "        break;\n",
      "      default:\n",
      "        skipRecursion = false;\n",
      "        break;\n",
      "      }\n",
      "    }\n",
      "    if (!skipRecursion) {\n",
      "      int child_count = ast.getChildCount();\n",
      "      for (int child_pos = 0; child_pos < child_count && phase1Result; ++child_pos) {\n",
      "        phase1Result = phase1Result && doPhase1(\n",
      "            (ASTNode)ast.getChild(child_pos), qb, ctx_1, plannerCtx);\n",
      "      }\n",
      "    }\n",
      "    return phase1Result;\n",
      "  }\n"
     ]
    }
   ],
   "source": [
    "t = data[data['changes_line'] == 522]\n",
    "for i in t['methods_before']:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
